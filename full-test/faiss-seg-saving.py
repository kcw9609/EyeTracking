from pathlib import Path
from PIL import Image
import numpy as np
import torch
import faiss
import json
import cv2
from transformers import CLIPProcessor, CLIPModel
from ultralytics import YOLO
import os

# ===========================
# ëª¨ë¸ ë¡œë“œ
# ===========================
model_name = "openai/clip-vit-base-patch32"
device = torch.device('cpu')
clip_model = CLIPModel.from_pretrained(model_name).to(device)
processor = CLIPProcessor.from_pretrained(model_name)

yolo_model = YOLO("yolov8n-seg.pt")  # YOLOv8 segmentation ëª¨ë¸

# ===========================
# ê²½ë¡œ ì„¤ì •
# ===========================
image_dir = Path("../test-images/")   # ì›ë³¸ ì´ë¯¸ì§€ í´ë”
index_save_path = "./faiss/image_clip.index"
meta_save_path = "./faiss/image_meta.json"
os.makedirs("./faiss", exist_ok=True)

# ===========================
# í•¨ìˆ˜ ì •ì˜
# ===========================
def segment_and_crop(image_path: Path):
    image = cv2.imread(str(image_path))
    if image is None:
        print(f"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return []

    image = cv2.resize(image, (1280, 720))
    results = yolo_model(image, conf=0.3)[0]

    if results.masks is None:
        return []

    masks = results.masks.data.cpu().numpy()
    cropped_images = []

    for i, mask in enumerate(masks):
        binary_mask = (mask > 0.5).astype(np.uint8)
        binary_mask = cv2.resize(binary_mask, (image.shape[1], image.shape[0]))
        binary_mask_3ch = np.stack([binary_mask]*3, axis=-1)

        masked_image = np.where(binary_mask_3ch == 1, image, 255)
        x_indices, y_indices = np.where(binary_mask == 1)

        if x_indices.size == 0 or y_indices.size == 0:
            continue

        x_min, x_max = np.min(y_indices), np.max(y_indices)
        y_min, y_max = np.min(x_indices), np.max(x_indices)
        cropped_object = masked_image[y_min:y_max, x_min:x_max]

        if cropped_object.size == 0:
            continue

        cropped_images.append((cropped_object, i))  # ì´ë¯¸ì§€ì™€ ì¸ë±ìŠ¤ ë°˜í™˜

    return cropped_images

def image_embedding(pil_img: Image.Image) -> np.ndarray:
    inputs = processor(images=pil_img, return_tensors="pt").to(device)
    with torch.no_grad():
        emb = clip_model.get_image_features(**inputs)
    emb = emb / emb.norm(dim=-1, keepdim=True)
    return emb.cpu().numpy().astype("float32")

# ===========================
# ë©”ì¸ ì‹¤í–‰
# ===========================
vectors, ids, descriptions = [], [], []

image_files = list(image_dir.glob("*.jpg")) + list(image_dir.glob("*.jpeg")) + list(image_dir.glob("*.png"))

for img_file in image_files:
    print(f"ğŸ” {img_file.name} ì²˜ë¦¬ ì¤‘...")
    crops = segment_and_crop(img_file)

    for cropped_object, crop_idx in crops:
        # OpenCV ì´ë¯¸ì§€ë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        pil_cropped = Image.fromarray(cv2.cvtColor(cropped_object, cv2.COLOR_BGR2RGB))
        try:
            emb = image_embedding(pil_cropped)
            vectors.append(emb)
            crop_id = f"{img_file.stem}_crop{crop_idx}.jpg"  # ì˜ˆ: image-1_crop0.jpg
            ids.append(crop_id)
            descriptions.append(f"Auto-generated crop from {img_file.name}")
        except Exception as e:
            print(f"Error embedding {img_file.name} crop {crop_idx}: {e}")

# ===========================
# FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥
# ===========================
if len(vectors) > 0:
    matrix = np.vstack(vectors)
    d = matrix.shape[1]
    index = faiss.IndexFlatIP(d)  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©
    index.add(matrix)

    faiss.write_index(index, index_save_path)
    print(f"âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_save_path}")

    meta_data = {"ids": ids, "descriptions": descriptions}
    with open(meta_save_path, "w") as f:
        json.dump(meta_data, f, indent=2, ensure_ascii=False)

    print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {meta_save_path}")
    print(f"ì´ {len(ids)}ê°œ crop ê°ì²´ ì¸ë±ì‹± ì™„ë£Œ")
else:
    print("â— ì„ë² ë”©ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
