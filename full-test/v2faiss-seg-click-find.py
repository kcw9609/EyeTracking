#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Apr 29 10:38:09 2025

@author: kangchaewon
"""

import cv2
import numpy as np
import torch
import faiss
from PIL import Image
from pathlib import Path
from ultralytics import YOLO
from transformers import CLIPProcessor, CLIPModel
import json
import os

# ===========================
# ëª¨ë¸ ë¡œë“œ
# ===========================
yolo_model = YOLO("yolov8n-seg.pt")
clip_model_name = "openai/clip-vit-base-patch32"
clip_model = CLIPModel.from_pretrained(clip_model_name)
clip_processor = CLIPProcessor.from_pretrained(clip_model_name)

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ
index = faiss.read_index("./faiss/image_clip.index")

# ===========================
# image_meta.json ë¡œë“œ (ìˆ˜ì •ëœ ë²„ì „)
# ===========================
with open("./faiss/image_meta.json", "r") as f:
    image_meta = json.load(f)

crop_id_list = image_meta["ids"]
descriptions = image_meta["descriptions"]
crop_id_to_description = {crop_id_list[i]: descriptions[i] for i in range(len(crop_id_list))}

# ===========================
# ì´ë¯¸ì§€ ì„ë² ë”© í•¨ìˆ˜ (PIL ì´ë¯¸ì§€ ì…ë ¥)
# ===========================
def image_embedding_from_pil(pil_img: Image.Image) -> np.ndarray:
    inputs = clip_processor(images=pil_img, return_tensors="pt")
    with torch.no_grad():
        emb = clip_model.get_image_features(**inputs)
    emb = emb / emb.norm(dim=-1, keepdim=True)  # L2 ì •ê·œí™”
    return emb.cpu().numpy().astype("float32")

# ===========================
# ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰ í•¨ìˆ˜
# ===========================
def search_similar_image(embedding: np.ndarray, index: faiss.Index, top_k=1):
    distances, indices = index.search(embedding, top_k)
    return distances, indices

# ===========================
# ê°ì²´ íƒì§€ ë° í´ë¦­ ì´ë²¤íŠ¸ í•¨ìˆ˜
# ===========================
def detect_and_interact(image_path: str):
    image = cv2.imread(image_path)
    if image is None:
        raise FileNotFoundError(f"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")

    screen_width, screen_height = 1280, 720
    image = cv2.resize(image, (screen_width, screen_height))

    results = yolo_model(image, conf=0.3)[0]
    class_names = yolo_model.names

    if results.masks is None:
        print("â— ê°ì²´ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    masks = results.masks.data.cpu().numpy()
    classes = results.boxes.cls.cpu().numpy().astype(int)

    np.random.seed(42)
    colors = np.random.randint(0, 255, size=(len(class_names), 3), dtype=np.uint8)

    seg_image = image.copy()
    object_regions = []

    for i, mask in enumerate(masks):
        class_id = classes[i]
        color = colors[class_id]
        binary_mask = (mask > 0.5).astype(np.uint8)
        binary_mask = cv2.resize(binary_mask, (image.shape[1], image.shape[0]))

        colored_mask = np.zeros_like(image, dtype=np.uint8)
        for c in range(3):
            colored_mask[:, :, c] = binary_mask * color[c]

        seg_image = cv2.addWeighted(seg_image, 1.0, colored_mask, 0.5, 0)
        object_regions.append((binary_mask, class_names[class_id]))

    os.makedirs("cropped_objects", exist_ok=True)

    def on_mouse(event, x, y, flags, param):
        if event == cv2.EVENT_LBUTTONDOWN:
            for mask, label in object_regions:
                if mask[y, x] == 1:
                    print(f"ğŸ–±ï¸ ({x}, {y}) â†’ '{label}' ê°ì²´ í´ë¦­")

                    binary_mask = mask

                    # 1. ë§ˆìŠ¤í¬ë¥¼ 3ì±„ë„ë¡œ ë³€í™˜
                    binary_mask_3ch = np.stack([binary_mask]*3, axis=-1)

                    # 2. ì´ë¯¸ì§€ì™€ ê³±í•´ì„œ ê°ì²´ ë¶€ë¶„ë§Œ ë‚¨ê¸°ê³  ë°°ê²½ì€ í°ìƒ‰ìœ¼ë¡œ ì±„ìš°ê¸°
                    masked_image = np.where(binary_mask_3ch == 1, image, 255)

                    # 3. ë§ˆìŠ¤í¬ ë¶€ë¶„ë§Œ ìë¥´ê¸°
                    x_indices, y_indices = np.where(binary_mask == 1)
                    x_min, x_max = np.min(y_indices), np.max(y_indices)
                    y_min, y_max = np.min(x_indices), np.max(x_indices)
                    cropped_object = masked_image[y_min:y_max, x_min:x_max]

                    if cropped_object.size == 0:
                        print("â— í´ë¦­í•œ ê°ì²´ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤.")
                        return

                    # í¬ë¡­í•œ ê°ì²´ ì €ì¥
                    save_path = f"cropped_objects/cropped_{x}_{y}.png"
                    cv2.imwrite(save_path, cropped_object)
                    print(f"ğŸ“· í¬ë¡­ëœ ê°ì²´ ì €ì¥ ì™„ë£Œ: {save_path}")

                    pil_cropped = Image.fromarray(cv2.cvtColor(cropped_object, cv2.COLOR_BGR2RGB))

                    embedding = image_embedding_from_pil(pil_cropped)
                    distances, indices = search_similar_image(embedding, index)

                    print(f"\nğŸ” ê°€ì¥ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ì¸ë±ìŠ¤: {indices[0][0]}")
                    print(f"ğŸ” ê±°ë¦¬(ìœ ì‚¬ë„ ì ìˆ˜): {distances[0][0]}")

                    if indices[0][0] < len(crop_id_list):
                        matched_crop_id = crop_id_list[indices[0][0]]
                        description = crop_id_to_description.get(matched_crop_id, "ì„¤ëª… ì—†ìŒ")
                        print(f"ğŸ“„ íŒŒì¼ëª…: {matched_crop_id}")
                        print(f"ğŸ“ ì„¤ëª…: {description}")
                    else:
                        print("â— ì¸ë±ìŠ¤ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")
                    return
            print(f"ğŸ–±ï¸ ({x}, {y}) â†’ ê°ì²´ ì—†ìŒ")

    cv2.namedWindow("YOLOv8 Segmentation Click")
    cv2.setMouseCallback("YOLOv8 Segmentation Click", on_mouse)

    while True:
        cv2.imshow("YOLOv8 Segmentation Click", seg_image)
        if cv2.waitKey(1) & 0xFF == 27:  # ESC í‚¤ë¡œ ì¢…ë£Œ
            break

    cv2.destroyAllWindows()

# ===========================
# ì‹¤í–‰ ë¶€ë¶„
# ===========================
if __name__ == "__main__":
    detect_and_interact("../test-images/image-3.jpg")
